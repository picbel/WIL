// Metadata:
:description: Week I Learnt
:keywords: study, til, lwil
// Settings:
:doctype: book
:toc: left
:toclevels: 4
:sectlinks:
:icons: font
:hardbreaks:


[[section-202504]]
== 2025년 4월

[[section-202504-2일]]
2일
===
### MySql의 User-Level-Lock (Named Lock)
Named Lock이라고도 불리며 MySql공식문서를 보면 아래와 같이 지원한다.
https://dev.mysql.com/doc/refman/8.0/en/locking-functions.html
사용법은 공식문서를 통해 확인하기

#### GET_LOCK()의 기본 동작
GET_LOCK(name, timeout)은 User-level Lock을 획득한다.
이 락은 **세션** 단위로 관리되며, 락을 획득한 세션이 종료되면 자동으로 해제된다.
락 정보는 In-Memory에 저장되고, DB에 Persist 되지 않는다.

User-Level-Lock의 경우 테이블이나 레코드가 아닌, 단순한 문자열을 기준으로 Lock을 획득한다.
비관적 락은 레코드 수준에 락을 걸기 때문에, 다른 작업들을 처리하기 위한 커넥션들도 락이 걸린 레코드에 접근하지 못하게 된다.
이러한 문제는 User-Level-Lock을 사용하면 발생하지 않는다.
정리하자면 기존의 데이터와 무관하게 Lock을 다룰수 있다.

또한 Lock임으로 반드시 해제(unlock)를 잊지 말아야한다.

#### **주의사항** 
##### Failover시 Lock 유실 가능성 있음

Replication과 GET_LOCK() — Master-Slave 환경
GET_LOCK()과 RELEASE_LOCK()은 **트랜잭션 로그(binlog)**에 기록되지 않는다.
즉, 이 명령어들은 Replication 되지 않는다.
Slave나 새로운 Master는 이전 Master에서 어떤 락이 잡혀 있었는지 알 수 없다.
따라서 Failover 시에는 다음과 같은 일이 일어난다
```
A 서버가 Master일 때 클라이언트가 GET_LOCK('foo')으로 락을 잡음.
A가 다운되고 B가 Master로 승격됨.
B에는 'foo' 락이 있었다는 정보가 없음.
결과적으로,
B에서 동일한 'foo' 락을 다시 획득할 수 있습니다. <- Lock 유실
```
GET_LOCK()으로 획득한 락은 Master 서버의 세션 메모리 내에만 존재한다.
따라서 Failover 후 새 Master에서는 해당 락이 존재하지 않음.
즉, 해제할 수도 없고, 락이 유지되지도 않음.
**즉 Failover 상황에서는 Lock 유실이 발생 할 수 있음**

##### 사용시 Lock 커넥션과 로직이 실행되는 커넥션(세션) 분리
스프링부트의 히카리 커넥션풀 기준 세션과 커넥션은 1:1 관계이다.
이 문제는 MySql의 mvcc, User-Level-Lock의 세션단위의 Lock이 얽히면서 발생하는 문제이다.
USER-LEVEL Lock을 사용한다면 Lock을 얻는 트랜잭션과 로직을 수행하는데 사용되는 커넥션을 분리시켜야 한다.
우선 왜 두 트랜잭션을 분리해야 하는지에 대해 알아보면 트랜잭션이 분리되지 않은 경우, 다음과 같은 상황이 발생할 수 있다.
```
Tx1 - 트랜잭션 시작
Tx2 - 트랜잭션 시작
Tx1 - GET_LOCK('stock-a')
Tx2 - GET_LOCK('stock-a') // TX1을 대기합니다
Tx1 - select * from Stock ... // 재고 테이블 조회 재고 1개
Tx1 - update ... // 재고 감소, 재고 0개
Tx1 - RELEASE_LOCK('stock-a'), COMMIT // unlock
Tx2 - 락 획득 후 select * from Stock ... // mvcc로 인해 재고가 1개로 조회됨
Tx2 - update ... // 재고 감소, 재고 0개
Tx1과 Tx2의 갱신손실 발생
```
위 처럼 트랜잭션이 동시에 실행되고 경합되었을때 시점에 따라 락 이후에 접근하는 데이터가 이전 데이터를 바라봐 동시성 문제가 발생할수 있음으로, 락 획득과 로직의 두 트랜잭션은 분리되어야한다.

---

[[section-202504-4일]]
4일
===
### 2PC (Two-Phase Commit)
**분산 트랜잭션에서 원자성(Atomicity)**을 보장하기 위한 프로토콜이다. 여러 시스템에 걸친 트랜잭션이 모두 성공하거나, 모두 실패하도록 보장한다.

동작 방식

1. Prepare Phase (준비 단계)
  Coordinator(조정자)가 각 참여자(participant)에게 “준비됐냐”고 물어봄 (prepare 요청)
  각 참여자는 자신의 트랜잭션 로그에 commit 가능 여부를 기록하고 응답함
2. Commit Phase (커밋 단계)
  모든 참여자가 “OK”하면 coordinator가 commit 명령을 내림
  하나라도 “NO”면 rollback 명령을 내림

장점
- 트랜잭션 원자성 보장

단점
- 동기적 처리로 인해 속도가 느림
- Coordinator 장애 시 blocking 발생 (참여자들이 커밋/롤백을 못하고 대기함)
- 장애 복구 어려움

추가로 2PC는 XA트랜잭션 지원이 필수적임
XA 트랜잭션이란?
**XA(Extended Architecture)**는
여러 자원 관리 시스템(Resource Managers, 예: DB, 메시지 큐 등) 간의 분산 트랜잭션을 지원하기 위한 표준이다.
Java에서는 javax.transaction.xa.XAResource 인터페이스를 통해 지원한다.
일반적으로 JDBC 드라이버, JTA 트랜잭션 매니저, 메시지 브로커 등이 이 표준을 따른다.

2PC는 다음과 같은 역할이 필요함:
Coordinator (트랜잭션 매니저): 전체 트랜잭션을 조율
Participant (리소스 관리자): 각 자원(DB, MQ 등)을 제어
→ XA 트랜잭션은 이 둘 사이의 통신을 표준화된 방식으로 제공해 주기 때문에, 2PC 구현을 가능하게 한다.
예: Atomikos, Narayana, Bitronix, Spring JTA 등은 XA 기반의 트랜잭션 매니저이다.

문제점
XA/2PC는 현실에서는 거의 잘 안 쓴다, 이유는 다음과 같음:
- 일부 시스템이 XA를 완전히 지원하지 않음 (예: 일부 NoSQL, 클라우드 서비스)
- 성능, 확장성 문제
- MSA 환경에서는 오히려 서비스 간 결합도를 높임

---

[[section-202504-5일]]
5일
===
### 블룸 필터(Bloom Filter)
**공간 효율적인 확률 기반의 자료구조**로,  
특정 값이 집합에 **"존재하지 않는지"를 빠르게 판단**하는 데 사용된다.

#### 동작 방식

1. **초기 상태**
   - `m`개의 비트로 구성된 배열 (모두 0으로 초기화)
   - `k`개의 서로 다른 해시 함수 사용

2. **값 추가 (put / add)**
   - 저장할 값에 대해 `k`개의 해시 함수를 적용
   - 각각의 해시 함수 결과값을 `m`비트 배열의 인덱스로 변환
   - 해당 인덱스 비트를 모두 `1`로 설정

3. **값 검사 (mightContain / exists)**
   - 검사할 값에 대해 `k`개의 해시 함수를 적용
   - 각 해시 결과값으로 비트 배열의 인덱스를 조회
   - **하나라도 0이면 → "절대 없음"**
   - **모두 1이면 → "있을 수도 있음" (오탐 가능성 있음)**

동작 예시
[cols="1,1", options="header"]
|===
| 데이터 추가 | 비트 배열 변화
| `apple` → hash1 → 3 | `[0, 0, 0, 1, 0, 0, ...]`
| `apple` → hash2 → 7 | `[0, 0, 0, 1, 0, 0, 0, 1, ...]`
| `apple` → hash3 → 20 | `[0, 0, 0, 1, ..., 1 (at 20)]`
|===
그다음 누가 `apple`을 조회할 때, hash1/hash2/hash3 결과 비트가 모두 `1`인지 확인함.


[cols="1,3", options="header"]
|===
| 항목 | 설명
| *추가(add)* | 추가는 가능, 여러 해시 → 각 해시값 위치 비트 = 1
| *조회(contains)* | 여러 해시 → 전부 비트 = 1 이면 "있을 수도 있음", 하나라도 0이면 "절대 없음"
| *삭제는?* | 일반적으로 불가능
| *오탐 가능성?* | 있음. 하지만 "없다"는 절대적으로 정확
|===

장점
- 매우 빠르고 메모리 효율적
- "없는 것"은 확실하게 걸러냄
- 해시 함수만 있으면 구현 가능

단점
- 오탐(false positive) 가능
- 삭제 불가 (기본 구현 기준)
- 오탐률 튜닝 필요 (`m`, `k`, `n` 조절)

오탐이 발생하는 이유는?
- 비트 배열을 공유하기 때문.
- 여러 키가 같은 위치의 비트를 1로 만들 수 있고, 그 때문에 "겹치는 영역"이 생긴다.
- 그래서 어떤 키가 실제로 저장된 적 없더라도, *다른 키들로 인해 비트가 1*인 상태일 수 있다.

실무에서 유용한 사용 예
- 존재하지 않는 유저/상품 요청을 빠르게 걸러내기
- 캐시 미스 최적화 (DB 접근 방지)
- 크롤링 중복 방지
- 추천 시스템 등에서 본 적 없는 ID 거르기

---

[[section-202504-6일]]
6일
===
### 아웃박스 패턴 (Outbox Pattern)
하나의 로컬 트랜잭션 안에서 **DB에 저장할 데이터와 함께 이벤트 메시지까지 같이 저장**하고,  
이후에 메시지 브로커로 메시지를 **비동기적으로 발행**하는 방식이다.

다음과 같은 상황을 방지하기 위해 사용되는 패턴이다
```
주문 서비스에서 주문 데이터를 DB에 저장한 후,  
Kafka를 통해 "주문 생성됨" 이벤트를 발행해야 함.
```
하지만 이걸 트랜잭션 없이 처리하면
1. DB 저장은 성공했는데 Kafka 발행 실패 → 데이터 불일치
2. Kafka 발행은 성공했는데 DB 저장 실패 → 이중 처리 위험
**DB와 메시지 브로커 사이의 트랜잭션 경계 문제**가 발생한다.

Outbox Pattern은 다음과 같이 동작함
1. 트랜잭션 내에서
- 주문 정보 저장
- **이벤트 내용을 `outbox` 테이블에 같이 저장**

2. 별도의 프로세스 또는 쓰레드가
- `outbox` 테이블을 폴링 (또는 CDC로 감지)
- 메시지를 브로커(Kafka 등)에 발행
- 발행 완료된 메시지는 삭제 or 상태 변경

장점
- 일관성 보장, DB와 메시지 전송을 **한 트랜잭션**으로 처리 가능
- 실패한 이벤트 재처리 가능
- 메시지 전송 로직을 서비스 로직과 분리할 수 있음
- 2PC보다 훨씬 빠르고 안정적

단점
- `outbox` 테이블 관리가 필요 (용량 증가, TTL 관리 등)
- 메시지 중복 방지 로직 필요 (idempotent 소비자 구현)
- Kafka 발행 실패 시 재시도 정책 구현 필요

구현 방식
- **Poll 방식**: 일정 간격으로 `outbox` 테이블을 폴링하여 메시지 발행
- **CDC 방식** (Change Data Capture): Debezium + Kafka Connect 사용하여 binlog 기반으로 outbox 이벤트 감지 → 실시간성 우수

